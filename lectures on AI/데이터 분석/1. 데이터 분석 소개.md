# R Vs. RStudio

- R: R interperter를 제공하는 순수 R 해석기
- RStudio: R을 사용하는 개발 환경



# 데이터 분석의 단계

1. 데이터 수집:  소스별로 데이터를 추출하고 저장하는 단계 
2. 데이터 전처리: 기초통계 + 붙이고, 없애고, 자르고, 채우고, 변경하는 단계
3. 데이터 정리: 데이터를 한곳에 담아두고(Data Warehouse) + 바꾸고 정리(Data Mart) + 분리(Data split)하는 데이트 관리
4. 데이터 분석: 기술통계 + 추론통계 + 모델링 + 검증 + 에러 분석
5. 결과 정리: 시각화 + 의사결정 + 추천 + 지식화 + 공유



> 각각의 단계는 완료라는게 없이, 지속적으로 반복



## 조금더 자세하게 본다면 ... (현실적으로)



1. 문제 정의: 무엇을 분석할지 정해야 하니깐요 ... 

> 무엇을 분석할지 사실 아무도 모른다. 
>
> 무엇을 분석할지는 모르지만 일단 도구는 다 준비한다. (플랫폼? R? Python? AWS?)미리 준비한다. 왜? 필요하다니깐
>
> 무엇을 분석할지 아무도 모르지만 일단 완료 계획과 일정이 모두 준비 되어 있다. 
>
> 시작은 했으니깐 이미 프로젝트는 시작되었다. 
>
> 그리고 데이터 분석 프로젝트는 이미 시작되었습니다. 라고 누군가에 의해 보고 됩니다. 

- 누군가 요구를 해요
  - 적당한 입사자를 찾아주세요 
  - 지금 주식을 사야하나요? 말아야 하나요? 
  - 마켓팅을 하기 위해서 누구한테 프로모션을 해야 하는지 알려줘봐~ 라고 듣기/이해하기/생각하기 전에 이미 프로젝트는 시작됩니다. 



2. 데이터 수집

> 수집할 데이터가 PC 에 있는줄 알았는데 어라? 종이로 있네?, 누군가에게 시켜서 파일로 바꾼다. 
>
> 어찌저찌 해서 데이터를 구하긴 했는데... 빅데이터라기에는 좀 그렇고 그냥 엑셀 파일 몇개 ... 
>
> 이 데이터로는 충분한지 충분하지 않은지 아무도 모릅니다. 
>
> 그래도 어쨋든 데이터는 구했으니깐 일단 시작합니다. 
>
> 그리고 보고는 빅데이터를 기반으로 데이터 수집이 완료되었다 라고 보고 된다. 근데 보고된 사실을 아무도 모릅니다. 



3. 데이터 전처리

> 무엇을 분석할지 모르지만, 일단 전처리는 합니다. 
>
> 일단 이상한건 지우고 보자, 어라? 남는게 없네? 
>
> 그냥 아무거나 채워본다. 왜? 어짜피 아무도 몰라요 아 내가 할 수 있는게 별로 없구나 라고 깨닫습니다. 
>
> 그리고 일단 데이터는 잘 처리되고 있다고 보고 됩니다. 근데 아무도 이 사실을 모릅니다. 



4. 데이터 정리

> 일단 플랫폼에 데이터를 올리긴 올린다. 
>
> 서버 사양도 체크하고, 일단 하는것 처럼 보인다. 
>
> 데이터가 플랫폼에 잘 올라갔으니 이제 곧 분석을 시작할거라고 보고가 됩니다. 근데 아무도 몰라요 



5. 데이터 분석

> 무엇을 분석하고, 무슨 데이터를 어떻게 쓸지 모르지만(아무도 몰라요) 일단 시작해요 
>
> 기술통계는 횟수 하나면 충분하다 
>
> 일단 쪼아서 제일 최신 알고리즘 적용해본다. 
>
> (잘 안된다 싶으면)... 회귀? 과적합? 어디서 들은것 있기 때문에 일단 해본다 
>
> 근데 더이상 할게 없음을 깨닫게 됩니다. 
>
> 분석이 완료 되었음이 보고 됩니다. 



6. 결과 정리 

> 보고서도 쓰고, 발표도 하고, 해야 됩니다. 
>
> 무엇을 분석하고 무슨 데이터를 어떻게 써야 하는지 모르지만 일단 결과를 정리합니다. 
>
> 내일이 완료일인데 퇴사를 해야하나 고민하기 시작합니다... 
>
> 그런데 정말 정말 신기하게 모든 단계는 잘 작동하고 구현되었다고 성과 보고를 한다. 
>
> 캐글의 데이터 분석과 현실의 분석은 다르구나 라는걸 깨닫게 됩니다. 



## 데이터 분석의 단계별 목적 



1. 문제 정의

> 문제 정의가 없으면 분석은 시작할 필요가 없다. 
>
> 정말 많은 시간을 들여서라도 아주 많은 고민을 해야 합니다. 
>
> 그리고 이 문제는 정말 지속적인 소통과 대화를 통해서(직원, 고객, 구성원, ... ) 끊임없이 발전시키고 변경 시켜야 한다. 
>
> 전체 과정에서 제일 많은 시간(공)을 들여야 하지 않나... 



2. 데이터 수집

> 데이터가 없으면 분석은 시작할 수 없다. 
>
> 정의된 문제에 대한 변수가 수집된 데이터에 없으면 분석을 할 수가 없다. 
>
> 	+ 네트워크 트래픽을 가지고 악의적인 트래픽인지를 구분: 트래픽 내에서 판단할 수 있는 변수를 추출하고, 검증하고 또 추출하고, ... 
> 	+ 데이터 분석은 양(row)보단 질(col)
> 	+ 데이터 수집부터 다시 시작해야 한다.(목적에 맞게)



3. 데이터 전처리

> 전처리의 목적은 여러가지가 있는데 
>
> 각 소스별 데이터를 하나로 모으고
>
> 모아진 데이터를 가지고 오류를 제거하고, 결측치를 채우거나, 없애거나, ... 
>
> 필요한 변수들만 추려낸다는 표현이 맞겠네요 ... 
>
> 사람이 이해 가능한 형태로 변경하고 

- 여기까지 전체 과정의 80% 이상 차지한다고 보면 맞다. 
- 1회성으로 끝나지 않고, 끊임없이 업데이트 하고, 진화하여야 한다. (이는 자동화된 플랫폼으로는 절대 처리가 불가능)
- 머신러닝/딥러닝 알고리즘은 자동화 할 수 있겠지만
- 실제로 알고리즘 트레이딩 하는 곳에서는 거의 매일 변수가 갱신된다고 카더라. 
- 항공, 군사, 날씨, 등등 이런곳에서는 이미 거의 10년 ~ 20년 동안 이런 변수들을 지속적으로 연구 되어왔다. 
  - 그래서 이 분야들에서는 머신러닝이 지금 효과적으로 운용되고 있다고 카더라. 



4. 데이터 정리

> 데이터에 대한 무결성을 지키는것이 1원칙으로 각각의 데이터를 목적에 맞게 관리하기 위한 단계
>
> 데이터 warehouse: 전처리를 거친 데이터를 보관 및 무결성 유지가 목적 
>
> 데이터 mart: 웨어하우스는 변경하지 않고 복사하여 목적에 맞게 전처리하고 분리 하는 단계
>
> 데이터 split: 학습 데이터와 평가 데이터로 구분하여 저장하고 알고리즘에서 사용



5. 데이터 분석

> 목적에 맞는 알고리즘 선정, 적용, 성능 확인 
>
> 수학적으로 어려울순 있지만 사실 라이브러리가 워낙 좋아서 대응및 활용이 어느정도 가능



6. 결과 정리 

> 1-5단계를 무한히 반복하면서 각 단계별로 인사이트를 찾을 수 있어야 한다. 
>
> 설명력을 위한 단계이기 때문에 정해진 방향은 없다고 보면 된다. 





# 머신러닝과 통계적 모형 

- 통계적 모형을 만드는 작업은 기계학습이 나오기 이전부터 있어 왔다. 
- 흑사병 발병 예측
  - 흑사병이 발생했던 당시에 통계
  - 주로 발생하는 지역이 지하도, 하수도 근처에서 발병율이 높게 발생하는 것을 통계를 통해서 확인
- 다변수라고 하지만, 변수 1, 2개 정도 ... 
- 회귀분석, 변수 1, 2개 => 변수 1000개, 10000개 100000개 이상( gpu가 있었기 때문에)
- 현재는 더 복잡 -> 변수들이 더 많아짐

- 머신러닝이라는 것은 통계적 모형의 또다른 표현



## 통계적 모형이라 함은 결국 수학적 모형

- 수학적 모형 

  - 주어진 입력변수 x와 출력변수 y 사이의 관계를 함수f로 표현

  $$
  y = f(x) \\
  y = b + ax
  $$

> 예를 들면,
>
> x는 지역, 집의 크기, 전기 사용량, 온도, ... 등 에 대해서
>
> 가능한 모든 경우의 수를 고려하여 최적의 $y=f(x)를 찾는 거죠



- 통계적 모형
  - y를 반응변수 라고 한다. 
  - x를 입력변수 하고 한다. 
  - 반응변수 y가 입력변수 x에 대해 함수 f(x)를 평균으로 하는 확률분포를 따른다고 가정

$$
y = f(x) + \epsilon
$$



- 주어진 입력변수 x와 반응변수 y사이의 함수 f에 대해 오차항의 존재를 가정



> 예). 동전 던지기 
>
> 동전 던지기에서 앞면과 뒷면이 나올 확률이 각가 1/2 로 잘 알려져 있다. (빈도로적 확률론/고전 확률)
>
> 동전을 던졌을 때, 앞면이 나오면 한걸음 앞으로 가고, 뒷면이 나온다면 한걸음 뒤로 간다고 해봅시다. 
>
> 0에서 출발한다고 가정, 동전을 100번 던지면, 현재 위치는 어디가 될 것인가?



## 통계적 추정과 머신러닝의 과정

- 통계적 추정 과정
  1. 확률분포를 이용한 통계적 모형으로 표현 
  2. 모형에 따른 `손실`을 부여한다. 
  3. 손실을 최소화 하는 방향을 모형을 적합한다.

- 머신러닝의 과정 
  1. 모델링: 기계의 형태를 결정
  2. 평가: 기계가 학습해야 할 방향을 결정
  3. 최적화: 기계를 튜닝하는 작업



## 용어의 차이



| 머신러닝                          | 통계                                             |
| --------------------------------- | ------------------------------------------------ |
| Feature(특징점)                   | Variable (변수)                                  |
| Y = Label(output_)                | 반응변수, 출력변수, 종속변수(Dependent Variable) |
| X = input                         | 입력변수, 독립변수(Independent Variable)         |
| 학습(Learning)                    | 추정(Estimation)                                 |
| 지도학습(Supervised Learning)     | 예측(Regression) / 분류(Classification)          |
| 비지도학습(Unsupervised Learning) | Clustering                                       |



# 통계와 확률

- 관찰된 자료(현상/상태)에 대해서 통계적으로(수치적으로) 처리하고 연구하는 학문
- 근데, 사실, ... 현상들이 통계에 의해서 움직인다면 얼마나 좋을까요 ... 
- 통계는 크게 기술통계와 추론통계로 나누어 볼 수 있다. 



## 기술통계

- 평균, 분산, 표준편차와 같은 기초통계량을 계산하여 자료의 성질(특성)을 확인(설명)하는 것
- 히스토그램의 통해 자료의 분포를 확인, 막대그래프를 통한 자료의 요약, ... 
- EDA(탐색적 자료 분석)는 기술 통계에 기반한 내용이다. 



## 추론 통계

- 머신러닝에서 보게되는 내용들은 전부 추론통계라고 보면 된다. 
- 표본을 통해 모집단을 추론하는 과정



### 모집단과 표본

![](https://phhp-faculty-cantrell.sites.medinfo.ufl.edu/files/2012/07/big_picture_producing_data.gif)



- 우리가 모집단을 알고 있으면 이런거 안해도 됩니다. 



> 예를 들어서
>
> 우리가 대한민국 전체 집값(모집단)에 대한 정보를 모두 알고 있다면 추론을 왜 하죠? 이미 알고 있는데
>
> 마케팅에 대한 전체(모집단)를 알고 있다면 왜 예측을 할까요? 이미 알고 있는데, 마케팅에 얼마를 쓰면 매출이 얼마 올라간다는 것을 이미 알고 있다는 건데 ... 
>
> 하지만, 결과적으로 모집단에 대해 모두 알고 있는것은 불가능(모집단에 대해서 아는 사람이 있다면, 신이겠죠)



- 그래서 우리는 추정을 합니다. 왜? 모집단을 모르니깐요 
- 표본을 통해서 모집단을 추정하는거죠



## 우리가 통계를 대하는 자세

- 마크 트웨인 
  - 거짓말의 3가지 종류: 거짓말, 새빨간 거짓말, 그리고 통계
- 왜 머신러닝이 인기가 많아졌을까요? 알파고의 역할도 있겠지만... 
- 의사 앞에 서면, ... 왜 그런지 모르겠지만 위축되는 경향이 있지 않나요? 의사 말이라면 다 맞는것 같고,
- 특히, 흰 가운 입고 얘기를 하면 왠지 신빙성이 있지 않나요? 판사가 판사 가운입고 얘기하면 더 무거워 보이고 
- 일반적으로 뭔가 복잡하고 어려운 과학기법을 사용했다고 하면 맹신하는 경향이 있는 것 같아요 
- 통계라고 하면 사람들이 잘 안믿는데 머신러닝이라고 하면 뭔가 맹신하는 경향이 있어요 

- 근데 이게 말이 되나? 상식적으로? 통계는 모집단의 모든 특징을 설명하지 못합니다. (일기예보는 항상 맞지 않죠)



### 통계의 한계

- 모집단의 특성을 표본을 통해서 통계로 설명하는 것은 뭔가 멋진 방법으로 보인다. 괜찮은 접근이기도 합니다. 
- 그러나, 표본의 특성이 꼭 모집단과 같을거라는 보장은 없다. (굉장히 위험한 발상)
  - 표본을 통해 모집단을 추정하는 방식이 좋은 방향인지 누구도 알 수 없다. 
- 하나의 표본으로 모집단의 평균을 정확히 알 수는 없지만, 추측은 가능하다. 
  - 빅데이터라는 건 결국 하나의 표본일 뿐, 빅데이터가 모집단을 대표할 순 없다. 모집단이 될 수도 없겠지만
- 추측이니깐 99%정도로 맞을 가능성도 있지만 1%의 틀렸을 가능성 또한 반드시 존재
- 표본을 어떻게 추출(수집)하느냐에 따라서 많은 것들이 변하겠죠... 
  - 표본이 왜곡되어 있을수도 있고, 만약 그렇다면 추정된 결과는 의미가 전혀 없을수도 있습니다. 
  - 관찰자의 주관이 깊이 관여할 수도 있기 때문에 이상적인 표본 추출은 거의 불가능 하다고 봐도 된다. 
- 모집단은 시간이 지나면서 변화할 수도 있다. 그러면 표본은? 
  - 즉, 과거에 추출한 표본은 현재를 잘 설명하지 못할수도 있다. 
  - 시간의 변화에 따라 모집단의 변화까지도 추정할 수 있으면 좋겠지만, ... 
- 우리가 하려는 건 과거의 데이터를 이용해서 현재나, 미래를 설명하려고 하는거다. 이게 말이 되는가? 
  - 어떻게 보면 정말 말도 안되는 작업이에요
- 사람들이 통계를 잘 믿지 못하는 것도 이해는 되죠



### 우리가 통계를 쓰지 않을수는 없다. 

- 일반적으로 통계가 아니고서는 대규모의 조사의 어려움을 해결할 수 없다. 
- 모집단을 설명할 수 있는 방법이 없어요 
- 자료간의 다소의 차이는 있을수 있지만 공통적인 특징을 확인할 수 있다. 
  - 코로나 백신의 효능을 증명하려면 통계적으로 유의미한지 비교를 통해서 통상적으로 미치는 특성은 확인이 가능
- 그리고, 꼭 예측이나 분류를 하는 용도로만 사용하는 건 아니다.





## 데이터가 가지는 의미? 

- 데이터가 표현하는 성질이 나보다 나를 더 잘 알고 있다(?)



> 예를 들어서 
>
> SNS에서 좋아요를 누른것만 확인을 해도 내가 뭘 좋아하고 뭘 싫어하는지 본인보다 더 잘 설명이 가능
>
> 일반적으로 사람들은 감정적이거나, 사실에 기반하기 보다는 상상을 통해서 자신을 설명
>
> 어떤 학생이 저에게 "저 되게 열심히 공부했어요" 라고 얘기 했다면
>
> 열심히 공부 했다는 기준이 뭘까요? 단순히 책상앞에 오래 앉아 있기만 했다면? 혹은 2시간 공부했는데 본인 스스로는 되게 열심히 했다고 느낄수도 있고요... 
>
> 그 학생의 휴대폰 정보를 수집해서 본다면 사실에 기반해서 "진짜 열심히 했구나"
>
> 나보다 나를 더 잘 안다는 거죠



- 이렇게 흥미롭지만 우리는 통계가 거짓말로 부터 출발한다는 사실을 알고 있어야 한다. 
- 그렇기 때문에, 스스로 입증해야만 하는거죠
- 분석을 잘 쓰면 아주 굉장한 무기가 되죠! 하지만 새빨간 거짓말 될 수도 있죠
- 주사위 문제: 주사위를 굴려서 관찰된 데이터를 쓴다고 하면 왜곡됐을까요? 안됐을까요?
  - 식사하시면서 생각 해보기시기 바랄게요
  - 결과적으로 얘기하면, 왜곡되었습니다. 왜? 굴린 주사위의 모양이 모두 똑같다고 얘기 할 수 있나요? 
  - 굴리다보면 한쪽이 마모 될 수도 있고, ... 또 괜히 힘들 다르게 보면 같은 방향으로 던져도 다르게 나오지 않을까요? 
  - 주사위 면의 길이가 정말 다 똑같은가? 공평하다고 얘기할 수 없습니다. 



# 기술 통계와 탐색적 데이터 분석



## 기술 통계



### 변수의 종류

- 질적변수 vs. 양적변수: 자료의 특성에 따라 분류
  - 질적변수(Qualitative Variable): 범주형, 비수치적이고, 특정 카테고리에 포함되는 변수(성별, 색상, ... )
    - 명목변수: 변수의 값이 특정한 범주에 들어가는 변수, 순위는 존재하지 않는다. (성별, 혈액형, .. )
    - 순위변수: 변수의 값이 특정한 범주에 들어가지만, 순위를 가지는 경우(성적, .. )
  - 양적변수(Quantitative Variable): 연속형, 변수의 값이 숫자로 나타나는 경우(키, 소득, 온도, ... )
    - 이산변수: 하나하나 셀 수 있는 변수(정수 형태, 나이, ... )
    - 연속변수:실수 형태의 값은 갖는 변수



- 독립변수 vs. 종속변수: 자료의 관계의 따른 분류 



### 통계량

- 자료로부터 계산되는 특성값을 뜻한다. 
- 탐색적 자료 분석에는 중심에 대한 통계와 산포에 대한 통계 등을 사용한다. 

- 중심에 대한 통계량, 산포에 대한 통계량, 관계에 대한 통계량



1. 중심에 대한 통계량(중심 통계량)

- 자료의 중심경향을 나타내는 수치 
- 평균: 자료의 평균, 표본의 중심 무게(산술평균, 기하평균, 조화평균, 가중평균, ... )
- 중앙값: 자료에서 상위 50%에 위치한 값, 자료를 정렬했을 때 중앙(50%)에 위치한 값
- 최빈값: 자료에서 가장 빈번하게 등장하는 값, 표본자료에서 가장 빈번한 값 



2. 산포에 대한 통계량, 변동 통계량

- 지표의 변동성을 나타내는 수치
- 자료의 퍼짐의 정도(?)로 이해하시면 좋을 것 같아요
  - 범위(Range): 최대값과 최소값의 차이
  - 편차(Deviation): 관측값과 평균의 차이 
  - 변동(Variation): 편차의 제곱의 합
  - 분산(Variance): 편차의 제곱의 합을 데이터의 수로 나눈 값
    - 표준편차(Standard Deviation): 분산의 제곱근


$$
\begin{array}{c|c}
	x \\
	30 \\
	26 \\
	22 \\
	\hline 
	78
	\end{array}
	Mean = 26
	
	\;\;\;\;\;\;\;\;
	\begin{pmatrix}
	x - \bar{x} \\
	4 \\
	0 \\
	-4 
	\end{pmatrix}
	\;\;\;\;\;\;\;\;
	\begin{pmatrix}
	(x - \bar{x})^2 \\
	16 \\
	0 \\
	16 
	\end{pmatrix}
	
	\;\;\; Variation = 32, \;\; Variance = 16, \;\;std = 4\\
	
	variance = \frac{\sum(x - \bar{x})^2}{n-1} = \frac{32}{2} = 16 \\
	standard\;deviation = S = \sqrt{16} = 4
$$


- 분산과 같은 경우의 자료의 퍼짐의 정도
- 표준편차가 작다는 것은 자료들 간의 차이가 굉장히 작다 ... 
  - 모든 데이터의 값이 동일하다면 표준편차는 0이다. 



- 기술통계에서 사용하는 분산, 표준편차 등의 값은 실제 값을 나타낼 수 없다. 
- 수학적 공식에 의해서 추출되는 값들일 뿐이다. 
- 수식에 의해서 도출되는 값이지 그게 실제 데이터의 어떤 특성을 의미하지 않는다. 
- 통계값만을 보고 데이터의 특성을 쉽게 단정 지어선 안됩니다. 



3. 관계에 대한 통계량(관계 통계량)

- 자료간의 관계를 나타내는 수치 
- 공분산: 두 변수의 잔차를 곱해서 더하고, 자료의 수만큼 나눈 것
- 상관관계: 두 변수의 변화에 대해서 표준화된 공분산
- 인과관계: 두 변수 중에 하나는 원인이 되고 하나는 결과가 되는 관계성



- 공분산

$$
\frac{\sum(x-\bar{x})(y-\bar{y})
}{n-1}
$$





- 상관관계 또한, 수치적인 계산에 의해서 나온 값일 뿐
- 실제로 이 상관계수가 정말 두 자료간의 상관성을 완벽하게 표현할 수 는 없다.
- 이건 숫자일 뿐, 실제 데이터에 대한 의미를 부여할 수는 없다. 



4. 형태 통계량

- 자료의 분포 형태와 왜곡(Skewness)을 나타내는 수치
- 왜도(Skewness): 편향성, 중심으로 좌우로 데이터가 편향되어 있는 정도
- 첨도(Kurtosis): 뾰족함의 정도
- 이상치(outlier): 평균을 벗어나는 값인데, ... 기준이 애매하죠 ... 








































# 최대가능도추정(Maximum likelihood estimation)



## 스팸메일 분류기

- 새로 도착한 메일이 스팸인지 안닌지 분류하는 기계를 생각해봅시다. 
- 입력변수는 `메일의 단어들` 이고 반응변수는 `스팸 여부` (0, 1)이라 할 수 있습니다. 
- 이진분류 문제가 되겠죠?  - Binary Classification 이라고도 부릅니다. 
- 이때, 생각해볼 수 있는 유용하게 사용될 수 있는 분포로는 `베르누이 분포` 가 있겠죠? 
- 우리의 모형을 다음과 같이 생각해본다면 

$$
y \sim Ber(f(x))
$$

- 메일이 들어온 단어 목록 x가 주어졌을 때, 그 메일이 스팸일 확률을 나타내는 f(x)로 나타낸 모형으로 볼 수 있겠죠? 
- 즉, 어떤 메일이 새로 들어왔을 때, f(x)가 0.7로 계산이 되었다면 0.7의 확률로 이 메일이 스팸 이므로, 스팸메일로 분류가 될 겁니다. 



## 가능도(likelihood)

- 각 면이 1,1,1,2,3,3으로 이루어진 1번 주사위와 1,1,2,2,3,3으로 이루어진 2번 주사위가 있다고 가정 합시다. 
- 어떤 주사위를 한 번 던졌을 때, 얻는 수를 x라고 했을 때,
- 관측된 숫자가 1 이었다고 해봅시다. 즉, x=1이 되는거죠?
- 어떤 주사위를 던졌다고 생각하는 것이 가장 합리적일까? 
  - 1번 주사위는 1이 나올 확률이 1/2 
  - 2번 주사위의 1이 나올 확률은 1/3

- 자료의 분포가 다음과 같다고 할 때, 

$$
(x_1, y_1), \cdots , (x_n, y_n) \; 분포가 \;f(x,y;\theta)
$$

- 모수 세타에 의존하는 자료가 있다고 했을 때, 다음과 같은 확률밀도함수의 곱으로 정의

$$
L(\theta;x,y) = \prod_{i=1}^n f(x_i, yi; \theta)
$$

- 앞의 주사위 문제에서 주사위의 번호를 k라고 합시다. 
- k에 대한 가능도는 다음과 같이 표현

$$
	L(k=1;x=1) = \frac{1}{2}, L(k=2;x=1) = \frac{1}{3}
$$

- 그래서, 관측된 숫자 x=1로 부터 우리가 얻는 정보를 고려해보면 1번 주사위가 던져졌을 거라고 생각하는것이 가장 합리적
- 자료로부터 계산된 가능도 함수를 최대화 하는 추정이 `최대가능도추정` 이라고 부릅니다. 
- 일반적으로 곱보다는 합이 더 쉽고, 곱으로만 계산을 하면 확률값이 작아지기 때문에 로그를 씌운 로그가능도를 사용하게 됩니다. 

$$
log(L(\theta)) = \sum_{i=1}^n log(f(x_i, y_i;\theta))
$$

- 손실함수의 목적은 손실을 `최소화` 하는 방향으로 하기 때문에 `-`를 취해서 손실을 최소화 하는 방향을 모형을 추정할 수 있게 된다. 
- `Negative log-likelihood` 라고 부릅니다. 

$$
- log(L(\theta)) = \sum_{i=1}^n -log(f(x_i, y_i;\theta))
$$



# Negative log-likelihood 



1. 최대가능도추정이란?
   - 확률밀도함수들의 곱으로 정의되는 모수에 대한 함수의 가능도
   - 이 가능도를 최대화 하는 추정을 `최대가능도추정` 이라고 합니다. 
   - `Maximum Likelihood Estimation` 이라고 부릅니다. 



2. Maximum Log-likelihood Estimation 이란? 
   - 일반적으로 곱 보다는 합이 더 계산하기가 쉽고, 확률들의 곱은 작아지기 때문에 
   - 로그를 취한 연산을 하게 되는데, 이를 `Maximum Log-likelihood Estimation` 라고 부릅니다. 



3. Negative Log-likelihood Estimation 이란?
   - 일반적으로 손실함수는 손실을 최소화 하는 방향으로 근사하기 때문에 
   - log-likelihood estimation에 부호를 취하여 최대값을 이용하는 최대가능도가 아닌 
   - 최소화 하는 방향으로 확인할 수 있다. 



# 최소제곱추정과 최대가능도추정



- 우리가 앞에서 선형회귀 모형을 적합하기 위해서 사용했던 손실 함수는 
  - LAD(Least Absolute Deviation) - 최소절대오차, 최소 절대 값, 최소 절대 잔차, ... 
    - 잔차의 절대값의 합 - 각 잔차들의 거리의 합
  - LSE(Least Squres Esimation ) - 최소제곱추정
    - 잔차의 제곱의 함 - 제곱의 합이란? 정사각형의 넓이와 같죠 .. 



- 최대가능도추정을 이용
  - 선형회귀를 적합 해볼 수 있습니다. 
  - 수학적으로는 제곱손실을 최소화 하는 방법과 최대가능도를 찾는 방법은 같은 방법임이 알려져 있다. 



# 전기사용량 

- 우리는 이전에 온도에 따른 아이스크림 판매량에 대한 회귀모형을 적합해본적이 있습니다. 

- 이 예제는 동일하게 온도에 따른 전기 사용량으로 바꿔본거에요 

- x를 온도, y를 전기 사용량이라고 가정 합니다. 

  - 날씨가 더워지면(온도가 높아지면) 사람들이 에어컨을 더 많이 틀 것이므로, 반응변수 y와 온도 x 사이에는 비례하는 관계가 있을 것이라고, 추정
  - 그래서, 이 관계를 절편이 베타0와 기울기인 베타1을 이용해 다음과 같이 표현

  $$
  y = \beta0 + \beta1 \cdot x + \epsilon
  $$

  

- 입력변수 X의 분포를 정규분포로 가정

$$
y \sim norm(x, \mu, \sigma)
$$

- 각각의 (x, y) 에 대해서 

$$
y_i = \beta_0 + \beta_1 \cdot x_i \;\;\ then \\
	y_i \sim N(\beta_0 + \beta_1\cdot x_i, \sigma)
$$

- 모수를 추정하려면, 다음의 가능도를 최대로 하는 모수 베타0와 베타1을 찾으면 된다. 

$$
L(\beta_0, \beta_1) = \prod_{i=1}^n dnorm(y_i, \beta_0 + \beta_1\cdot x_i, \sigma)
$$

- `Negative log-likelihood`의 손실을 최소화 하는 모수를 찾도록 하기 위해서 다음과 같이 정의될 수 있다. 

$$
-log(L) = -\sum_{i=1}^n log(dnorm(y_i, \beta_0 + \beta_1\cdot x_i, \sigma)
$$



# 분류모형



### 스팸 메일 분류기

- 새로 도착한 메일이 스팸메일인지 아닌지를 분류하는 기계를 생각해보자. 
- 입력변수를 어떤걸로 할 것인가?는 매우 중요한 문제 이지만, ... 
- 메일에 있는 단어들로 일단 가정을 하겠습니다. 그렇다면 입력변수는 '메일의 단어들'이고, 
- 반응변수는 '스팸메일의 여부'인 '0, 1'이라고 할 수 있죠



## 종류

1. 출력변수를 0 또는 1과 같이 그 자체로 예측하는 방법
   - 도착한 메일에 `특정단어`가 있으면 스팸메일로 구분하는 방법
   - 결정트리(Decision Tree), KNN, SVM, ... 
   - 좋은 결정규칙이나 연관규칙을 찾는 문제가 된다.



2. 출력변수가 해당 집단에 속할 확률을 예측하는 방법
   - 도착한 메일에 `특정단어`가 있으면 스팸메일일 확률이 0.7이라고 한다. 
   - 계산한 확률을 바탕으로 출력변수의 집단을 예측 => 원 핫 인코딩
   - 나이브 베이즈 분류기, 로지스틱 회귀 등이 있다. 
   - 이산적인 값인데, 확률을 이용해서 연속적인 값을 추정하는 문제가 된다. 
   - 즉, 분류 문제이지만 회귀문제로 바꾸어 생각할 수 있게 된다.



### 직접분류

- 스팸메일 분류기의 단어목록에 대해서 

$$
	x_{list} = (x_1, x_2, x_3, ... , x_n)
$$

- 스팸메일 분류기를 정의 한다면 

$$
f(x) = \begin{cases}
		1\;\;\;\; if \; \beta_0 + \beta_1 \cdot x_1 > 0 \\
		0 \;\;\;\; if \; \beta_0 + \beta_1 \cdot x_1 \le 0
	\end{cases}
$$

- 이러한 모형은 첫 번째 단어만 보고 그 메일이 스팸인지 아닌 구분하게 된다. 
- 예를 들어서, 베타0 = -1 이고, 베타1 = 1 이라고 가정하면, 첫 번째 단어가 있으면 스팸이 되고, 그렇지 않으면 스팸이 아닌게 되죠
- 정확이 이진분류 모형이 되는거죠



### 확률을 예측하는 방법

- 모형을 다음과 같이 나타낸다고 본다면

$$
	Y \sim Ber(f(x))
$$

- 메일이 들어온 단어 목록 x가 주어졌을 때, 그 메일이 스팸메일일 확률 f(x)로 나타내는 모형
- 예를 들어서 어떤 메일이 새로 들어왔을 때 f(x)가 0.7로 계산되었다면, 

$$
	\mathbb{P}(y=1|x) = 0.7 \\
	\mathbb{P}(y=0|x) = 0.3
$$

- 이 메일은 0.7의 확률로 스팸이고, 0.3의 확률로 스팸이 아니다. 
- 베이즈 분류기는 스팸으로 분류하게 될 것
  - cut of value = 1/2 = 0.5



### 분류모형의 손실

- 0-1 loss

$$
l(y, \hat y) = \begin{cases}
		1 \;\;\; if \; y \neq \hat y \\
		0 \;\;\; if \; y = \hat y
	\end{cases}
$$

- 예측이 맞으면 0, 틀리면 1을 부여하는 손실

- 스팸메일을 스팸메일이 아닌 메일로 분류한 수 + 스팸메일이 아닌 메일을 스팸메일로 분류한 수 
- 즉, 손실이 0이라면, 스팸메일을 스팸메일로, 스팸메일이 아닌 메일을, 스팸이 아닌 메일로 잘 분류했다고 판단 



# 스팸 메일의 확률을 추정

- 주어진 입력변수 x에 대해서 확률을 계산하는 함수는 다음과 같이 정의해 볼 수 있다. 

$$
f(x) = \mathbb{P}(y=1|x)
$$

- 가장 간단하게는 다음과 같은 선형모형으로 생각해볼 수 있다. 

$$
f(x) = \beta_0 + \beta_1 \cdot x_1
$$

- 예를 들면, 첫 글자의 빈도를 이요해 주어진 이메일이 스팸메일일 확률을 계산해보는 경우를 생각해 볼 수 있다. 
- 첫 글자의 빈도수를 나타내는 x1에 대한 계수 beta1은 첫글자가 1번 이상 발생했을 때의 스팸이 될 확률값으로 바꿔서 생각해 볼 수 있다. 
- 그러면, 다음과 같이 확률이 추정이 되었다고 가정 해봅시다. 

$$
	f(x) = 0.2 + 0.7 \cdot x_1
$$

- 즉, 첫 글자가 1번 발생할 때마다 0.7의 확률로 스팸일 확률이 늘어난다라고 해석 가능하다. 
- 그런데, ... 첫 글자의 빈도가 3번이라면 2.3으로 확률값이 1보다 커지기 때문에 확률이 아니게 된다. 
- 그래서... 적당한 변화을 통해서 0과 1사이의 값으로 변환시키는 방법을 생각해봐야 한다. 



## 일반화 선형모형

- 다음과 같이 모형을 만들어 보자 . 

$$
	\mathbb{P}(y=1|x) = F(\beta_0 + \beta_1 \cdot x_1)
$$

- 함수 F(x)는 연속이며, 증가하고 0과 1사이의 값을 갖는 함수이다. 



### activation function

- logit

$$
F(x) = \frac{e^x}{1 + e^x}
$$

- probit

$$
	F(x) = \mathbb{P}(Z \le x), \;\; Z \sim norm()
$$



- logit fuction은 `누적확률분포함수` 이다. 

![](https://nathanbrixius.files.wordpress.com/2016/06/sigmoid_thumb.png?w=636&h=480)

- 그림의 형태만 잘 기억해 둡니다. 



## 일반화 선형모형에서의 추정

- 최소제곱추정

$$
	L = \sum_{i=1}^n (y_i - F(\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n))^2
$$



- 최대가능도추정

$$
	y \sim Ber(f(x)), \;\;\; f(x) = F(\beta_0 + \beta_1x_1 + \cdots + \beta_nx_n)
$$

- cross entory를 이용한 손실함수의 정의 

$$
L = - \sum_{i=1}^n (y_ilogf(x_i) + (1-y_i)log(1-f(x_i)))
$$

- R에서는 glm() 함수를 통해서 일반화 선형모형을 추정할 수 있다. 



## 로지스틱 회귀분석

- logit 함수를 사용하는 일반화 선형모형을 `로지스틱 회귀모형` 이라고 한다. 
- 로지스틱 회귀 모형의 이름은 여기서 유래가 되었습니다. 

- 찾고자 하는 모수 베타들에 대해서 다음과 같이 추정이 됐다고 한다면

$$
	x = (x_1, x_2, ... ,x_n)가 \; 주어지면 \; 반응변수 \; y_0가 \; 1일 \; 확률은  \\ 
	\mathbb{P}(y_0=1|x) = F(\hat\beta_0 + \hat\beta_1x_1 + \cdots + \hat\beta_nx_n)
$$

- 베타햇이 양수라는 건 x[i]가 증가할 때, y가 1이 될 확률이 증가한다는 것을 의미하고, 
- 베타햇이 음수라는 건 x[i]가 증가할 때, y가 1이 될 확률이 감소한다는 것을 의미합니다. 



### 오즈비

- 오즈비(odds ratio)
  - x가 한 단위 증가할 때 y = 1일 확률과 y = 0일 확률의 비가 증가하는 양을 의미
- 로지스틱 회귀 모형에서 회귀계수는 x의 로그 오즈비를 뜻하는 겁니다



# 결론 

- 스팸메일 분류기가 다음과 같이 모형을 추정했다고 가정 하면, 

$$
	F(0.3x_1 + 0.7x_2 + 0.9x_3 -1.2x_4)
$$

- 네 번째 회귀계수가 -1.2로 음수이므로 네 번째 단어가 많이 들어갈 수록 
- 스팸일 확률이 줄어든다 라고 해석될 수 있는 거죠
- 로지스틱 회귀 모형은 해석력이 좋고 수치적으로 계산이 쉬워서 널리 사용이 됩니다. 
- 통계적인 이론을 바탕으로 어떤 변수가 유의한지, 그렇지 않은지 등을 분석할 수 있고, 
- 선형관계를 바탕으로 만들었기 때문에, 직관적으로 구조를 파악하기 쉽다는 장점이 있다. 
- 변수간의 복잡한 관계를 표현하기에는 적합하지 않다.



# simple and scalable response prediction for display advertising

- 광고업계에서 아주 유명한 회사가 있어요 - criteo
- 이 회사가 얼마나 유명하냐면 
  - 실제로 머신러닝의 정확도가 높으려면 데이터의 양이 많아야 한다고 알려져 있어요 
  - 그런데 데이터의 양으로 치자면 사실 구글보다 더 많은 데이터를 가지고 있는 찾기 어렵겠죠?
  - 그럼에도 불구하고 마케팅으로 벤치마킹 해보면 60전 60패라고 해요(구글이)
  - 알고리즘을 승부하는 회사중에 하나인데 정확도 정말 잘 나온다고 하더라구요 
  - 근데 어떻게 그런 정확도를 가져갔는지에 대한 즉, 알고리즘에 대한 핵심 내용은 절대 공개를 하지 않는다고 합니다. 
  - 특허도 안내는거에요, 회사의 아주 극소수의 몇명만이 알고있다라고 하는 카더라가 있어요
- 이 논문은 그 회사에서 같이 일했던 동료 3명이 쓴 논문이에요
- 실제로도 유명한 논문이에요 
  - 데이터 분석 파트에 신입 채용 공고에서 5편 정도의 논문을 적어놓고 면접때 그 중에 1편을 골라서 리뷰하는 과제
  - NHN, ... 이었나 잘은 모르겠는데, ... 그 중에 한편이 이 논문이었어요



## 이 논문이 아주 훌륭하고 공부를 해봐도 좋을 이유

1. 일반적으로 그냥 딥러닝이 최고야 라는 인식이 전반적으로 많이 있죠
   - 실제로도 딥러닝이 성능이 좋긴 좋아요
   - 그렇지만 딥러닝도 단점이 있어요 - 무조건 딥러닝이 옳다(?)라고 얘기할 수는 없어요
   - 이 논문에서는 가장 최신의 알고리즘을 이용하지 않고도, 가장 기본이라고 할 수 있는 
   - 로지스틱 회귀모형을 통해서 충분히 효과적으로 적용할 수 있음 보였습니다. 
2. 실제로 모형을 만들때 중요한건 알고리즘이 아닌 데이터
   - Feature Engineering에 상당히 많은 부분을 할애를 합니다. 
   - 그리고 실제로도 마케팅 분야에서 이미 Feature들에 대해서 많은 연구가 있었어요... 
   - 즉, 데이터의 중요성을 너무 잘 알고 있는것 같아요 
   - 알고리즘이 다가 아니라는 거죠!
3. 데이터 수집, 분석, 모델링 , 적용까지 이 논문 한편에 다 들어가 있어요 
   - 어떻게 수집하고, 분석하고 어떤 알고리즘왜 왜 어떻게 적용했고, 
   - 문제가 뭐였고, 어떻게 해결했고, 실제로 적용(야후)까지 다 들어가 있어요
4. 단점은 
   - 기술적 난이도가 비교적 높아요: 강화학습, 머신러닝 시스템을 구축(하둡)
   - 그리고 가장 큰 단점은 영어에요! 읽기 무지 힘들었어요 
   - 이 논문은 제대로 읽고 싶어서 라인 by 라인으로 해석을 했었는데 진짜 힘들었어요 
5. 딥러닝의 단점
   - 일단 해석이 안되요 
   - 그냥 결과만 봐야 되요, 즉, 결과적으로 스팸이다 아니다만 확인이 되요 
   - 왜? 스팸으로 분류 했는지는 알 수 없어요
   - 예를 들어서 99%의 정확도로 결과를 내도 신뢰가 잘 안가요
   - 가장 큰 단점은 엄청난 연산을 필요로 한다는 것 - 아주 높은 시스템 사양을 요구하죠





















